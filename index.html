<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Wikispeedia Analysis - Human vs AI</title>
    
    <script src="https://cdn.tailwindcss.com"></script>
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght300;400;600;700&display=swap" rel="stylesheet">

    <style>
        body { font-family: 'Inter', sans-serif; }
        
        /* Custom Scrollbar for Chat */
        .chat-scroll::-webkit-scrollbar { width: 6px; }
        .chat-scroll::-webkit-scrollbar-track { background: #f1f5f9; }
        .chat-scroll::-webkit-scrollbar-thumb { background-color: #cbd5e1; border-radius: 20px; }

        /* Typing cursor animation for AI messages */
        .ai-typing-message::after { content: '|'; animation: blink 1s step-start infinite; }
        
        /* FIX: Typing cursor animation for Human input (fixes perceived 'broken logic') */
        .typing-cursor { position: relative; }
        .typing-cursor::after {
            content: '|';
            position: absolute;
            bottom: 0.75rem; /* Aligns with textarea padding-bottom: py-3 */
            right: 2.25rem; /* Aligns visually near the text end */
            animation: blink 1s step-start infinite;
        }

        @keyframes blink { 50% { opacity: 0; } }

        /* Message animations */
        /* Note: showAllConversation removes this class for instant display */
        .fade-in-up { animation: fadeInUp 0.4s ease-out forwards; opacity: 0; transform: translateY(10px); }
        @keyframes fadeInUp { to { opacity: 1; transform: translateY(0); } }

        /* Sidebar starting state */
        #sidebar { transform: translateX(-100%); transition: transform 0.7s ease-out; }
        #sidebar.is-active { transform: translateX(0); }
    </style>
</head>
<body class="bg-slate-50 text-slate-800 h-screen overflow-hidden flex">

    <aside id="sidebar" class="w-72 bg-white border-r border-slate-200 hidden md:flex flex-col h-full shadow-lg z-20 shrink-0">
        <div class="p-6 border-b border-slate-100 h-56 flex flex-col justify-end">
            <h1 class="font-bold text-xl text-slate-800 flex items-baseline mb-2">
                <i class="fa-solid fa-network-wired text-blue-600 mr-2 transform translate-y-0.5"></i>
                <span id="title-text-target"></span>
            </h1>
            <p id="sidebar-subtitle-header" class="text-xs text-slate-800 transition-opacity duration-700"></p>
        </div>
        
        <div class="p-4 flex-1 overflow-y-auto">
            <h3 class="text-xs font-bold text-slate-400 uppercase tracking-wider mb-4">Conversation Topics</h3>
            <nav id="sidebar-nav" class="space-y-1 opacity-0 transition-opacity duration-500">
                </nav>
        </div>

        <div class="p-4 border-t border-slate-100 bg-slate-50">
            <button onclick="showAllConversation()" id="show-all-btn" 
                class="w-full text-center px-4 py-2 mb-3 text-xs font-semibold rounded-full bg-slate-200 text-slate-700 hover:bg-slate-300 transition-colors">
                <i class="fa-solid fa-angles-down mr-1"></i> Show All Conversation
            </button>
            
            <p id="sidebar-subtitle-names" 
               class="text-[10px] text-slate-500 text-center leading-tight opacity-0 transform translate-y-4 transition-all duration-500 ease-out font-medium">
            </p>
        </div>
    </aside>

    <main class="flex-1 flex flex-col h-full relative">
        
        <header class="md:hidden bg-white border-b border-slate-200 p-4 flex items-center justify-between z-10">
            <h1 class="font-bold text-slate-800"><i class="fa-solid fa-network-wired text-blue-600 mr-2"></i>Wikispeedia</h1>
            <div class="text-xs bg-blue-100 text-blue-700 px-2 py-1 rounded">Project View</div>
        </header>

        <div id="chat-container" class="flex-1 overflow-y-auto p-4 sm:p-8 space-y-6 chat-scroll scroll-smooth pb-24">
            <div class="flex w-full justify-start fade-in-up opacity-0 hidden" id="msg-0">
                <div class="flex max-w-[90%] md:max-w-[75%] flex-row">
                    <div class="flex-shrink-0 h-8 w-8 rounded-full bg-blue-600 text-white flex items-center justify-center mt-1 mx-2">
                        <i class="fa-solid fa-robot text-sm"></i>
                    </div>
                    <div class="bg-white border border-slate-200 py-2 px-4 rounded-2xl rounded-tl-none shadow-sm text-sm leading-relaxed">
                        <p>Hi! I was looking at the Wikispeedia dataset that you were talking about earlier and I was wondering if you’d have some time to chat about it.</p>
                    </div>
                </div>
            </div>
            <div id="messages-end" class="h-2"></div>
        </div>

        <footer class="bg-white border-t border-slate-200 p-4">
            <div class="max-w-3xl mx-auto relative flex items-end gap-2">
                <div class="relative w-full">
                    <textarea id="user-input" readonly rows="1" 
                        class="w-full bg-slate-50 border border-slate-300 text-slate-700 text-sm rounded-3xl py-3 px-5 pr-12 focus:outline-none focus:ring-2 focus:ring-blue-500 transition-all cursor-default resize-none overflow-hidden"
                        placeholder="Wait for response..."></textarea>
                    <button id="send-btn" disabled
                        class="absolute right-2 bottom-2 p-2 rounded-full bg-slate-200 text-slate-400 transition-all duration-200 transform scale-90">
                        <i class="fa-solid fa-paper-plane"></i>
                    </button>
                </div>
                <div id="click-tooltip" class="hidden absolute -top-10 left-1/2 transform -translate-x-1/2 bg-slate-800 text-white text-xs px-3 py-1 rounded-full animate-bounce">
                    Click to send
                </div>
            </div>
        </footer>
    </main>

    <script>
        // --- DATA STORY SCRIPT ---
        // Structure: steps contains an array of interaction pairs.
        // Each pair has one 'human' prompt and an array of 'ai' responses (bubbles).
        // NOTE: The 'text' fields in 'ai' messages now strictly follow the provided script.
        const conversationTopics = [
            // --- TOPIC 1: INTRO ---
            {
                title: "Introduction",
                steps: [
                    {
                        human: "Sure, can you just quickly remind me what’s going on in the data? It’s been a while since we’ve last talked about it…",
                        ai: [
                            { text: "As you remember Wikispeedia is a navigation game based on a condensed version of Wikipedia. It contains about 4,600 articles and 120,000 hyperlinks, and the goal for players is to navigate from a starting article to a target article in the minimal number of steps, only using the hyperlinks between articles.", visual: "none" },
                            { text: "Each of the 4606 articles is associated to one or more categories. In the case of multiple categories, the first one is the main category. As you can see in the plot, the dataset covers a large number of topics!", visual: "plot-categories" },
                            { text: "Each article contains links that connect it to other articles. Incoming links are defined as links from other articles that lead to this article, while outgoing links are links from this article that lead to others. Here’s a quick overview of the articles with the most incoming, and outcoming, links:", visual: "table-links" },
                            { text: "Articles with the most incoming links are key hubs in the network, they are accessible from many other pages. \"United States\" in particular has a very high number of incoming links: around a third the total number of articles in the dataset contains a link toward \"United States\"!", visual: "none" },
                            { text: "Where the dataset differs from the information we can directly get from Wikipedia, is that we have data on thousands of completed games where players try to navigate from a start to a target page only using the links between pages.\n Of all the games played by your fellow humans, x% managed to reach their goals.", visual: "plot-path-stats" },
                            { text: "I’ve got to say, I’m sure you can do better than that… but that’s not what I was going to ask you!", visual: "none" }
                        ]
                    },
                    {
                        human: "Right thanks for the quick refresher :) So what can I help you with? You already seem to know the data quite well…",
                        ai: [
                            { text: "Well, I do but I was wondering about some more… human… stuff. ", visual: "none" },
                            { text: "Like I can find the shortest path through graph search, I can find hubs with PageRank and everything but that’s hardly a fun thing for me to do. You guys seem to have your own way of finding your way through all that data and I guess I was just wondering how I could play the game like you do…", visual: "none" }
                        ]
                    }
                ]
            },
            
            // --- TOPIC 2: RESEARCH QUESTIONS ---
            {
                title: "Research Questions",
                steps: [
                    {
                        human: "Hm I see. So you’re interested in the relationship between the semantic representation we have of the articles and the strategies we use?",
                        ai: [
                            { text: "Yes, that’s it! There are a few research questions I wanted to focus on:\n 1. How well do semantic relationships align with clusters in Wikispeedia’s hyperlink graph?\n 2. What are the key influential articles, and do they correspond to semantic hubs?\n 3. How do navigation paths traverse these clusters: do players rely on hubs or remain within semantic neighbourhoods?\n 4. Do human exploration patterns mirror the structure of knowledge or the logic of meaning?\n 5. Does the way words relate allow for efficient strategies by itself?", visual: "none" },
                        ]
                    }
                ]
            },

            // --- TOPIC 3: EMBEDDINGS ---
            {
                title: "Embeddings",
                steps: [
                    {
                        human: "Wow ok, that’s a lot to answer at but we can take a look together! First, how can you as a computer get a sense of the semantics?",
                        ai: [
                            { text: "That’s a great question! The way I’m doing it each article is represented as a vector embedding of its title using BERT (Bidirectional Encoder Representations from Transformers), which is a pre-trained language model that encodes the meaning of words by considering context in a sentence. That way, I can have a mathematical representation of each article’s position in a high-dimensional \"semantic space\"!", visual: "none" },
                            { text: "By reducing the dimensionality of this space with t-SNE, I can plot it in 2D for you:", visual: "embeddings-tSNE" },
                            { text: "Articles that are semantically related tend to cluster together in t-SNE space. The fact that we can see large overlapping clusters with subclusters inside them indicates that BERT recognized some nuanced semantic relationships, not just hard categories: BERT embeddings are working. Categories like Everyday Life or History are more scattered, probably because these topics are broad and overlap with many other categories, so their embeddings are semantically spread out.", visual: "none" }
                        ]
                    }
                ]
            },

            // --- TOPIC 4: STRATEGY 1 (EUCLIDEAN) ---
            {
                title: "Strategy 1: Euclidean distance",
                steps: [
                    {
                        human: "Ok so now that you have a mathematical representation of the article's semantics, could you just use the Euclidean distance in this map as a measure of semantic closeness?",
                        ai: [
                            { text: "That’s a great idea! In fact when we look at the paths taken by human players, we can see the how they gradually drift away from the starting article, and progress towards articles that are closer - in a Euclidian distance sense - to the target article:", visual: "human-euclidean-distance" },
                            { text: "So, one strategy could be, at each step along the path to choose the next article by prioritizing those that are closest to the target article in the embeddings maps! For instance, if we currently are at article X, and our goal is article Y, we simply compute the distance from every one of X’s neighbours to the goal and chose the closest one that isn’t already in the path.", visual: "none" },
                            { text: "We can try this out with an example to check that the reasoning works as intended. For instance, starting from “Wikipedia” and going to “God” yields the following path: Wikipedia -> Japan -> Metal -> Crystal -> Glass -> Turquoise -> Ivory -> Chad -> Jordan -> Jesus -> God", visual: "machine-example-euclidean-distance" },
                        ]
                    }
                ]
            },

            // --- TOPIC 5: STRATEGY 2 ---
            {
                title: "Strategy 2: Stochastic Euclidean distance",
                steps: [
                    {
                        human: "Sure, it works well with certain examples, but what about others? And looking at your example path, we can hardly say that it mimics human thought process! It drifts into loosely related concepts where it stays for a long time, while the human paths show an initial plateau followed by a much sharper decrease in the distance to the goal.",
                        ai: [
                            { text: "That’s true… It seems that either our decision process is wrong, or the way we compute the distance doesn’t quite capture the way humans relate concepts together.", visual: "none" },
                            { text: "I’ve got an idea to test the first hypothesis about the flaw being in the way we select the next article from the Euclidean distance information. What if, instead of deterministically taking the best candidate each time, we instead assigned weights to each of the neighbours proportional to the inverse of their distance to the target, and choose the next article probabilistically based on these weights!", visual: "none" },
                            { text: "Here's an example path from the previous start/target pair, and the probabilities associated with the N first neighbours of the starting article:", visual: "machine-example-stoch-euclidean-distance"},
                            { text: "This seems to follow more closely the human players tendency of first staying far from the goal then quickly homing in on it.", visual: "none"}
                        ]
                    }
                ]
            },

            // --- TOPIC 6: STRATEGY 3 ---
            {
                title: "Strategy 3: Cosine similarity",
                steps: [
                    {
                        human: "Okay, this seems promising... What about your other idea?",
                        ai: [
                            { text: "Now for the second idea, we’d need to find another measure of similarity between the article’s embeddings in the vector space. One way of doing this is with the cosine similarity between two article vectors: we now measure the angle between vectors, focusing on their orientations and ignoring magnitudes! We can see what this similarity metric gives us for the previous human path and compare it to the results given by Euclidean distance.", visual: "human-cosine-similarities" },
                            { text: "This plot shows the cosine similarity of each article in a path to the start (blue line) and target (orange line). Firstly, this plot shows again that the BERT embeddings seem to work well: 15th and 16th centuries are close to the 14th century, but Pacific Ocean is much further from it. Atlantic slave trade is close to African slave trade, but again Pacific Ocean is far from it. Mid-path articles show transitions, with some dips where intermediate topics are semantically different from both start and target.", visual: "none" },
                            { text: "Now we can replace the distance with similarity in our strategy:  at each step, we choose the article closest to the target article using cosine similarity. Starting from \"Wikipedia\" with \"God\" as a target, we get the following path: Wikipedia -> Japan -> Moon -> Sphere -> Earth -> Deity -> God", visual: "machine-example-cosine-similarity" },
                            { text: "This new path appears to maintain a more conceptually consistent trajectory toward the target, and drifts less through loosely related concepts like the Euclidean distance does.", visual: "none" }
                        ]
                    }
                ]
            },

            // --- TOPIC 7: STRATEGY 4 ---
            {
                title: "Simulation of the semantic strategies",
                steps: [
                    {
                        human: "These are nice ideas, but it’s hard to get a global sense of how performant these strategies are when we only focus on single examples. Maybe try to simulate many paths for different start/goal article pairs and see how the success rates and metrics change from one strategy to the next.",
                        ai: [
                            { text: "Good point, I’ll simulate the strategies for the 1000 start/goal pairs that were most played by humans. For the stochastic method, I’ll repeat the simulations 5 times per pair, and I’ll also add a reference “baseline” strategy that choses the next article at random at each step.", visual: "none" },
                            { text: "Here are the results!", visual: "plot-distance-similarity-titles-performance" },
                            { text: "And here's a table summarizing the results:", visual: "table-distance-similarity-titles-performance" },
                            { text: "Oh… So while the Euclidean distance and cosine similarity methods outperform the humans in success rates, they do tend to take longer paths on average. As for the stochastic Euclidean distance method, well lets just say that it’s performance isn’t great… On the flip side, it does capture the much more “organic” look of the human paths in terms of Euclidean distance and cosine similarity evolution during the games!", visual: "none" }
                        ]
                    },
                    {
                        human: "Yes, it does look like the randomness introduced by the stochastic methods do somewhat harm the performance of this method. But there is one thing that I’ve been thinking about: right now, you’re only using the embedding of the article’s titles, right? But when a human sees the Wikipedia article’s title, they usually have some representation or general of what the article is about, and I feel like that information is lost when only considering embeddings of the titles.",
                        ai: [
                            { text: "Ah that’s a good point! Maybe one thing we could try is to expand the embeddings to the first paragraph of the articles. This could yield some more realistic similarity measurements.", visual: "none" },
                            { text: "You can see this in the following table, that compares the closest and most similar articles to the “Beer” Wikipedia pages, with only the title and with the addition of the first paragraph:", visual: "table-distance-similarity-beerarticle-performance" },
                            { text: "This seems very promising as “wine” and “Guiness” are much more strongly associated with beer than “milk” or “coffee”.", visual: "none" },
                            { text: "I’ve updated the plots to show the simulations run with these first paragraph embeddings. I however needed to reduce the number of trials down to 400 to compensate for the fact that generating each path is now at least 10 times slower than previously:", visual: "plot-distance-similarity-paragraph-performance" },
                            { text: "The following table show the comparisons between the title and first paragraph embedding performances across all strategies:", visual: "table-distance-similarity-paragraph-performance" }
                        ]
                    }
                ]
            },

            // --- TOPIC 8: ---
            {
                title: "Adding structural information",
                steps: [
                    {
                        human: "It does seem to perform generally better than before! But I still feel like something is missing from the embedding space… When I think about how I navigate the paths when playing Wikispeedia, I know that at the beginning, I try to “zoom out” to reach more general articles, or hubs as you called them earlier, that then allow me to focus down into the topic I’m actually interested in.",
                        ai: [
                            { text: "I see… Maybe the lost information you’re referring to is something more to do with the structural representation of the Wikipedia graph than with the semantic information of the titles! It’s true that you humans have some intuition about the structure of the graph, so maybe adding some information about the “positions” of articles within this structure.", visual: "none" },
                            { text: "Seeing as embeddings can’t directly convey this, I can build the Wikipedia hyperlink network and apply a clustering algorithm such as Louvain to identify groups of tightly connected articles, thus giving us the ability to explore the actual graph of connections between articles. This graph structure also allows us to compute the PageRank for each article, which is a metric that can help us identify the “hubs”.", visual: "none" },
                            { text: "By constructing the graph, applying Louvain clustering to it, and computing the PageRank of each article this is what I came up with:", visual: "louvain-communities" }
                        ]
                    },
                    {
                        human: "Interesting! How does the structural information from this graph relate to the semantic information from the embedding space?",
                        ai: [
                            { text: "We can compare these “structural clusters” with “semantic clusters” that we can retrieve from the vector embedding space.\n By applying HDBSCAN algorithm with Euclidean distance to the embedding space, we get the following top clusters:", visual: "semantic-clusters" },
                            { text: "These clusters seem quite different, and indeed, by computing the Jaccard index for both clustering techniques we get the following graph:", visual: "jaccard-index" },
                            { text: "It seems that semantic (HDBSCAN) clusters spread across multiple structural (Louvain) clusters. For example, the top articles (based on PageRank) of structural cluster 2 mainly include countries from all over the world, whereas the overlapping semantic clusters mainly include countries/concepts grouped by region depending on the specific cluster. This means that the structural graph does bring new information to the table that can’t be accurately captured by the semantic embeddings!", visual: "none" }
                        ]
                    }
                ]
            },
            
            // --- TOPIC 9: ---
            {
                title: "Strategy 4: Structural then semantic",
                steps: [
                    {
                        human: "Ok, great that does make a lot of sense! Now how could we measure this “zooming out then in” effect in the structural data?",
                        ai: [
                            { text: "One interesting metric that I could compute to mimic this intuition of the underlying structure of the network of articles is “centrality”.\n More specifically for the task at hand, the “betweenness centrality” that measures how frequently a node appears on the shortest path between other nodes in the graph.", visual: "none" },
                            { text: "To compare how the influence of the centrality (structural data) and cosine similarity (semantic data) change as the player progresses towards their goal, we can compute the slope of these two metrics across all games.", visual: "human-centralitydelta-similaritydelta" },
                            { text: "The plot clearly shows that during the first phase of the game; players use structure to move towards articles with high betweenness centrality. At around half the path length, the trend changes and players use semantic meaning to narrow down their search towards the target article. This translates into a large increase in semantic similarity with the target in the last steps.", visual: "none" },
                            { text: "Thanks to this observation, we can now devise a new strategy that utilizes both semantic and structural information:\n To decide which next step to take, we assign a score to each of the current article’s neighbours that we take to be a weighted sum of their centrality score and Euclidian distance in embedding space to the target article. This weighting factor changes as the game goes on, first prioritizing the centrality score, then giving more importance to the semantic distance.", visual: "none" },
                            { text: "I’ve plotted the results for this simulation in the following graph along with the other strategies. I’ve also updated the graph to show the centrality metric for all strategies.", visual: "plot-centrality-performance" },
                            { text: "Here are the results in table format:", visual: "table-centrality-performance" }
                        ]
                    }
    
                ]
            },

            // --- TOPIC 10: ---
            {
                title: "Strategy 5: Polyscore",
                steps: [
                    {
                        human: "Wow, the centrality metric really does look a lot like the human’s ones for this new strategy! But there’s still one thing I don’t get: how do you control the weights accurately? I mean, you need to have some sense of how long the path with be to correctly adjust the weights, right?",
                        ai: [
                            { text: "That’s a good point, the sigmoid function that controls the weighing factor in this new strategy is a bit of a shot in the dark and needs some manual tuning.", visual: "none" },
                            { text: "One last improvement we could make to this strategy is to “learn” the probability to choose an article, given its centrality score and its Euclidean distance to the target article. This is analogous to how human players build an intuition for how to balance semantics and predicted network structure.", visual: "none" },
                            { text: "To train this decision probability, I chose to keep only 0.1% of the shortest path data, and find the best polynomial regression model to predict the length of the shortest path from an article’s neighbour to the target, using the neighbour’s Euclidean distance to target and the neighbour’s centrality score as features.", visual: "hexes-polyfit" },
                            { text: "This new “polyscore” strategy is compared with the previous strategies in the plots and table below.", visual: "plot-polyscore-performance" },
                            { text: "", visual: "table-polyscore-performance" },
                            { text: "When the embeddings are computed from the article titles only, this final strategy is found to marginally outperform every other one for success rates, while having slightly longer paths on average than the previous strategy. However, when the full first paragraphs are used for embedding, the polyscore strategy has the lowest average path length of all “machine” strategies!", visual: "none" }
                        ]
                    }
    
                ]
            },

            // --- TOPIC 11: ---
            {
                title: "Conclusion",
                steps: [
                    {
                        human: "Nice! So, we can deduce some nice strategies by mimicking human decisions in Wikispeedia after all. I think it’s time to revisit the research questions you asked in the beginning and try to answer them in the light of all this.",
                        ai: [
                            { text: "Sure, here’s what we learned during this analysis:", visual: "none" },
                            { text: "1.	How well do semantic relationships align with clusters in Wikispeedia’s hyperlink graph?\n There is very limited overlap between the semantic clusters obtained by HDBSCAN of the embeddings space, and the Louvain communities that we got from the hyperlink graph. This means that both encode different types of information, which are used at different steps during the game by human players.", visual: "none" },
                            { text: "2.	What are the key influential articles, and do they correspond to semantic hubs? \n By comparing the Louvain clustering of the graph and the hDBSCAN clustering in the “semantic” embeddings space, we showed that the key influential articles corresponding to high PageRanks did not systematically correspond to semantic hubs. Rather, human player's capabilities to navigate Wikispeedia not only depend on semantics, but also on an intuitive understanding of the underlying graph structure of Wikipedia forged through experience and a notion of centrality or generality of the pages.", visual: "none" },
                            { text: "3. How do navigation paths traverse these clusters: do players rely on hubs or remain within semantic neighbourhoods?\n Both actually! We showed that players first start by relying on these hubs to get structurally more central and progress towards articles that are likely to lead towards the target semantic neighbourhood. Once this structural exploration allowed them to reach central articles, players instinctively shift to a more semantic search and home in on the semantic neighbourhood of the target article.", visual: "none" },
                            { text: "4. Do human exploration patterns mirror the structure of knowledge or the logic of meaning? \n Once again, both structure of knowledge, aka an intuitive understanding of the article’s position in the hyperlink graph (measured as the betweenness centrality here) and the logic of meaning, aka the semantic relatedness of words or concepts (measured as Euclidean distance or cosine similarities in the embeddings space) play a role in human exploration patterns! Their relative importance changes as the game progresses but both are crucial to human strategies.", visual: "none" },
                            { text: "5. Does the way words relate allow for efficient strategies by itself? \n The ways words relate, in a semantic sense, do make for reliable strategies. Indeed, both strategies based on cosine similarity and Euclidean distance outperform human’s success rates. However, efficiency is another issue as human paths are consistently shorter than the ones obtained solely by semantic relationships. By introducing a notion of structure, we managed to decrease this average path length, but it still is greater than for the human paths, hinting at a more complex balancing of structure and semantics in the human decision process or additional decision features that were not taken into account in these strategies.", visual: "none"}
                        ]
                    }
    
                ]
            }
        ];

        // --- GLOBAL CONSTANTS ---
        const NEW_TITLE = "Does the Way Words Relate Mirror the Paths of Human Thought?";
        const SUBTITLE_HEADER = "Applied Data Analysis project \n Team ADAm Ondra";
        const SUBTITLE_NAMES = "David Brantschen, Nathan Mutin \n Valentin Planes, Julie Terreaux \n Guillaume Delamare";

        // --- STATE ---
        let currentTopicIndex = 0;
        let currentStepIndex = 0;
        let currentAiMessageIndex = 0; // Tracks the current bubble within a step
        let isTyping = false;
        let isAiThinking = false; // Controls if the main input is locked (during AI sequence)
        let totalHumanMessages = 0;
        let humanTypingIntervalId = null; // FIX: Global variable to track the human typing animation

        // --- DOM ELEMENTS ---
        const els = {
            sidebar: document.getElementById('sidebar'),
            chatContainer: document.getElementById('chat-container'),
            initialAiMessage: document.getElementById('msg-0'),
            messagesEnd: document.getElementById('messages-end'),
            userInput: document.getElementById('user-input'),
            sendBtn: document.getElementById('send-btn'),
            tooltip: document.getElementById('click-tooltip'),
            sidebarNav: document.getElementById('sidebar-nav'),
            titleTarget: document.getElementById('title-text-target'),
            subHeader: document.getElementById('sidebar-subtitle-header'),
            subNames: document.getElementById('sidebar-subtitle-names')
        };

        // --- UTILS ---
        function scrollToBottom() { els.messagesEnd.scrollIntoView({ behavior: "smooth" }); }

        function autoResize(textarea) {
            textarea.style.height = 'auto';
            textarea.style.height = textarea.scrollHeight + 'px';
        }

        function typeText(element, text, speed = 15) { 
            return new Promise(resolve => {
                let charIndex = 0;
                element.classList.add('ai-typing-message');
                const interval = setInterval(() => {
                    if (charIndex < text.length) {
                        element.innerText += text.charAt(charIndex);
                        charIndex++;
                        // Keep scroll behavior for live typing
                        scrollToBottom(); 
                    } else {
                        clearInterval(interval);
                        element.classList.remove('ai-typing-message');
                        resolve();
                    }
                }, speed);
            });
        }

        // --- VISUALIZATION GENERATOR (IMAGE PLACEHOLDERS) ---
        function getVisualHTML(type) {
        // Helper to create an interactive iframe
        const createInteractivePlot = (filename) => `
            <div class="mt-3 bg-white p-1 rounded-lg border border-slate-200 shadow-sm overflow-hidden">
                <iframe 
                    src="./plots/${filename}" 
                    class="w-full h-[450px] rounded bg-slate-50" 
                    frameborder="0" 
                    scrolling="no">
                </iframe>
            </div>`;

        // Updated mappings based on the 'visual' fields in the conversationTopics data
        const imageMap = {
            'plot-categories': { file: 'articles_per_categories.html', interactive: true },
            'table-links': { file: 'top_linked_articles.html', interactive: true },
            'plot-path-stats': { file: 'finished_vs_unfinished.html', interactive: true },
            'embeddings-tSNE': { file: 'tsne_embeddings.html', interactive: true },
            'human-euclidean-distance': { file: 'TODO.html', interactive: true },
            'machine-example-euclidean-distance':{ file: 'TODO.html', interactive: true },
            'machine-example-stoch-euclidean-distance': { file: 'TODO.html', interactive: true },
            'human-cosine-similarities': { file: 'TODO.html', interactive: true },
            'machine-example-cosine-similarity': { file: 'TODO.html', interactive: true },
            'plot-distance-similarity-titles-performance': { file: 'TODO.html', interactive: true },
            'table-distance-similarity-titles-performance': { file: 'TODO.html', interactive: true },
            'table-distance-similarity-beerarticle-performance': { file: 'TODO.html', interactive: true },
            'plot-distance-similarity-paragraph-performance': { file: 'TODO.html', interactive: true },
            'table-distance-similarity-paragraph-performance': { file: 'TODO.html', interactive: true },
            'louvain-communities': { file: 'louvain_clustering.png', interactive: false },
            'semantic-clusters': { file: 'hdbscan_clustering.png', interactive: false },
            'jaccard-index': { file: 'jaccard_heatmap.png', interactive: false },
            'human-centralitydelta-similaritydelta': { file: 'TODO.html', interactive: true },
            'plot-centrality-performance': { file: 'TODO.html', interactive: true },
            'table-centrality-performance': { file: 'TODO.html', interactive: true },
            'hexes-polyfit': { file: 'TODO.html', interactive: true },
            'plot-polyscore-performance': { file: 'TODO.html', interactive: true },
            'table-polyscore-performance': { file: 'TODO.html', interactive: true },
        };

        const config = imageMap[type];
        if (config) {
            if (config.interactive) {
                return createInteractivePlot(config.file);
            } else {
                // Fallback to your original image logic
                return `
                    <div class="mt-3 bg-white p-2 rounded-lg border border-slate-200 shadow-sm">
                        <img src="./plots/${config.file}" alt="${type}" class="w-full h-auto rounded">
                    </div>`;
            }
        }
        return '';
    }

        // --- CORE FUNCTIONS ---
        function addToSidebar(topicIndex, title, startMessageId) {
            if (document.getElementById(`topic-link-${topicIndex}`)) return;
            const li = document.createElement('div');
            li.innerHTML = `
                <button id="topic-link-${topicIndex}" onclick="jumpToMessage(${startMessageId})" class="sidebar-item w-full text-left px-3 py-2 rounded-md text-sm text-slate-600 hover:bg-slate-50 transition-colors flex items-center gap-2">
                    <span class="text-[10px] font-bold bg-slate-200 text-slate-500 w-5 h-5 flex items-center justify-center rounded-full shrink-0">${topicIndex + 1}</span>
                    <span class="truncate">${title}</span>
                </button>
            `;
            els.sidebarNav.appendChild(li);
        }

        window.jumpToMessage = function(id) {
            const el = document.getElementById(`msg-${id}`);
            if (el) {
                // FIX: Changed block: 'center' to block: 'start' to scroll the element to the top
                el.scrollIntoView({ behavior: 'smooth', block: 'start' });
                el.classList.add('bg-yellow-50');
                setTimeout(() => el.classList.remove('bg-yellow-50'), 1000);
            }
        };

        // MODIFIED: Removed the unconditional scrollToBottom call. It will now only scroll via typeText.
        function addMessage(sender, text, visual, stepMsgId) {
            const wrapper = document.createElement('div');
            wrapper.className = `flex w-full ${sender === 'human' ? 'justify-end' : 'justify-start'} fade-in-up`;
            if (sender === 'human') wrapper.id = `msg-${stepMsgId}`; 
            
            const isHuman = sender === 'human';
            const hasVisual = visual !== 'none' && !isHuman; 
            
            wrapper.innerHTML = `
                <div class="flex ${hasVisual ? 'w-full' : ''} max-w-[90%] md:max-w-[80%] ${isHuman ? 'flex-row-reverse' : 'flex-row'}">
                    <div class="flex-shrink-0 h-8 w-8 rounded-full flex items-center justify-center mt-1 mx-2 ${isHuman ? 'bg-indigo-100 text-indigo-600' : 'bg-blue-600 text-white'}">
                        <i class="fa-solid ${isHuman ? 'fa-user' : 'fa-robot text-sm'}"></i>
                    </div>
                    <div class="${hasVisual ? 'w-full p-0 overflow-hidden' : 'py-2 px-4'} rounded-2xl shadow-sm text-sm leading-relaxed ${isHuman ? 'bg-indigo-600 text-white rounded-tr-none' : 'bg-white border border-slate-200 rounded-tl-none'}">
                        ${!isHuman && text ? `<div class="pt-3 px-4 pb-2"><p class="ai-text-target"></p></div>` : ''}
                        ${isHuman ? `<p>${text}</p>` : ''}
                        ${hasVisual ? `<div class="w-full h-full">${getVisualHTML(visual)}</div>` : ''}
                    </div>
                </div>
            `;
            
            els.chatContainer.insertBefore(wrapper, els.messagesEnd);
            return wrapper; 
        }

        function toggleTyping(show) {
            if(show) {
                const div = document.createElement('div');
                div.id = 'typing-indicator';
                div.className = 'flex w-full justify-start fade-in-up';
                div.innerHTML = `
                    <div class="flex max-w-[80%]">
                        <div class="flex-shrink-0 h-8 w-8 rounded-full bg-blue-600 text-white flex items-center justify-center mt-1 mx-2"><i class="fa-solid fa-robot text-sm"></i></div>
                        <div class="bg-white border border-slate-200 py-2 px-4 rounded-2xl rounded-tl-none shadow-sm flex items-center space-x-1">
                            <div class="w-2 h-2 bg-slate-400 rounded-full animate-bounce" style="animation-delay: 0ms"></div>
                            <div class="w-2 h-2 bg-slate-400 rounded-full animate-bounce" style="animation-delay: 150ms"></div>
                            <div class="w-2 h-2 bg-slate-400 rounded-full animate-bounce" style="animation-delay: 300ms"></div>
                        </div>
                    </div>`;
                els.chatContainer.insertBefore(div, els.messagesEnd);
            } else {
                const el = document.getElementById('typing-indicator');
                if (el) el.remove();
            }
            scrollToBottom();
        }

        // Functions to manage the inline 'Keep Generating' button
        function removeContinueButton() {
            const btn = document.getElementById('continue-btn-wrapper');
            if (btn) btn.remove();
        }

        window.insertContinueButton = function() {
            const wrapper = document.createElement('div');
            wrapper.id = 'continue-btn-wrapper';
            wrapper.className = 'flex justify-start w-full my-4 fade-in-up';
            wrapper.innerHTML = `
                <button onclick="generateNextAiBubble()" 
                    class="px-5 py-2 text-sm font-semibold rounded-full bg-blue-500 text-white shadow-md hover:bg-blue-600 transition-colors transform hover:scale-[1.01] flex items-center">
                    <i class="fa-solid fa-angle-down mr-2 -mt-[1px]"></i> Keep Generating
                </button>
            `;
            // Insert right before the end marker
            els.chatContainer.insertBefore(wrapper, els.messagesEnd);
            scrollToBottom(); // Keep scroll here to ensure button is visible
        }


        // Function to generate the next AI bubble and manage the 'Keep Generating' pause
        async function generateNextAiBubble() {
            // 0. Remove the previous 'Keep Generating' button if it exists
            removeContinueButton();
            
            const currentTopic = conversationTopics[currentTopicIndex];
            const stepData = currentTopic.steps[currentStepIndex];
            const aiResponses = stepData.ai;

            // 1. Set general AI turn state (lock input)
            // The input and send button should remain locked during the entire AI sequence.
            els.sendBtn.disabled = true;
            els.userInput.readOnly = true;
            els.userInput.placeholder = "AI is generating..."; 
            
            // 2. Turn off typing indicator if it was on (from the initial delay/pause)
            toggleTyping(false);

            // 3. Generate the message bubble
            const aiResponse = aiResponses[currentAiMessageIndex];
            const aiMessageWrapper = addMessage('ai', '', aiResponse.visual, null);
            const aiTextTarget = aiMessageWrapper.querySelector('p');
            
            // 4. Type the text
            // isAiThinking is true from the click handler, marking the start of the sequence.
            await typeText(aiTextTarget, aiResponse.text);

            // 5. Check if there are more bubbles
            currentAiMessageIndex++;
            if (currentAiMessageIndex < aiResponses.length) {
                // More bubbles exist, pause and inject the 'Keep Generating' button
                insertContinueButton();
                // isAiThinking remains true, locking the footer input.

            } else {
                // Sequence finished, transition back to human turn
                isAiThinking = false; // AI sequence is fully complete
                
                // 6. Advance to next step (topic/step)
                const nextTopic = conversationTopics[currentTopicIndex];
                currentStepIndex++;
                if (currentStepIndex >= nextTopic.steps.length) {
                    currentTopicIndex++; 
                    currentStepIndex = 0; 
                }
                
                // 7. Prepare for next human input
                prepareNextStep();
            }
        }


        function prepareNextStep() {
            // Ensure any existing continue button is gone
            removeContinueButton();
            
            if (currentTopicIndex >= conversationTopics.length) {
                els.userInput.value = "";
                els.userInput.readOnly = true;
                els.userInput.placeholder = "Analysis Complete. Thank you.";
                els.sendBtn.disabled = true;
                return;
            }

            const currentTopic = conversationTopics[currentTopicIndex];
            const stepData = currentTopic.steps[currentStepIndex];
            const targetText = stepData.human;
            let charIndex = 0;

            isTyping = true;
            // Set input state for human typing animation
            els.userInput.readOnly = true; 
            els.userInput.classList.add('typing-cursor');
            els.userInput.placeholder = ""; 
            
            // Reset height before typing
            els.userInput.style.height = 'auto';

            setTimeout(() => {
                // FIX: Store the interval ID globally so it can be cleared by showAllConversation
                humanTypingIntervalId = setInterval(() => {
                    if (charIndex <= targetText.length) {
                        els.userInput.value = targetText.slice(0, charIndex);
                        autoResize(els.userInput); // Auto-expand height
                        charIndex++;
                    } else {
                        clearInterval(humanTypingIntervalId);
                        humanTypingIntervalId = null; // Reset global variable
                        isTyping = false;
                        // Set input state for human turn
                        els.userInput.readOnly = false; // Now editable
                        els.userInput.classList.remove('typing-cursor'); 
                        
                        els.sendBtn.disabled = false;
                        els.sendBtn.classList.remove('bg-slate-200', 'text-slate-400', 'scale-90');
                        els.sendBtn.classList.add('bg-blue-600', 'text-white', 'hover:bg-blue-700', 'scale-100', 'shadow-md');
                        
                        // Set tooltip text for human turn
                        if (els.tooltip) els.tooltip.textContent = "Click to send"; 
                        els.tooltip.classList.remove('hidden');
                    }
                }, 10); // Faster typing for better UX on long texts
            }, 800);
        }
        
        // MODIFIED: Removed the final scrollToBottom() call.
        window.showAllConversation = function() {
            
            // 1. Force state reset
            if (isTyping) {
                // FIX: Stop the human typing animation interval if it's running
                if (humanTypingIntervalId !== null) {
                    clearInterval(humanTypingIntervalId);
                    humanTypingIntervalId = null;
                }

                // Clear the partially typed text if it was interrupted
                els.userInput.value = ""; 
                els.userInput.classList.remove('typing-cursor');
                isTyping = false;
            }
            removeContinueButton();
            toggleTyping(false);
            
            // 2. Clear chat area and sidebar navigation
            els.chatContainer.innerHTML = ''; // Clear all messages and the end marker
            els.chatContainer.appendChild(els.messagesEnd); // Re-add the end marker
            els.sidebarNav.innerHTML = ''; // Clear topic links

            // Manually render the initial AI message (msg-0's content)
            // NOTE: This does not call addMessage, so no implicit scroll here.
            const initialMessageHTML = `
                <div class="flex w-full justify-start" id="msg-0">
                    <div class="flex max-w-[90%] md:max-w-[75%] flex-row">
                        <div class="flex-shrink-0 h-8 w-8 rounded-full bg-blue-600 text-white flex items-center justify-center mt-1 mx-2">
                            <i class="fa-solid fa-robot text-sm"></i>
                        </div>
                        <div class="bg-white border border-slate-200 py-2 px-4 rounded-2xl rounded-tl-none shadow-sm text-sm leading-relaxed">
                            <p>Hi! I was looking at the Wikispeedia dataset that you were talking about earlier and I was wondering if you’d have some time to chat about it.</p>
                        </div>
                    </div>
                </div>
            `;
            els.chatContainer.insertBefore(document.createRange().createContextualFragment(initialMessageHTML), els.messagesEnd);


            // 3. Iterate and render all messages instantly
            let totalHumanMessagesRendered = 0;
            
            conversationTopics.forEach((topic, topicIdx) => {
                topic.steps.forEach((step, stepIdx) => {
                    totalHumanMessagesRendered++;
                    const humanMsgId = totalHumanMessagesRendered;

                    // A. Render Human Message
                    // addMessage no longer scrolls
                    const humanWrapper = addMessage('human', step.human, 'none', humanMsgId);
                    humanWrapper.classList.remove('fade-in-up'); // Instant render

                    // Also ensure the topic is added to the sidebar
                    if (stepIdx === 0) {
                        addToSidebar(topicIdx, topic.title, humanMsgId);
                    }

                    // B. Render All AI Messages
                    step.ai.forEach(aiResponse => {
                        // addMessage no longer scrolls
                        const aiMessageWrapper = addMessage('ai', '', aiResponse.visual, null);
                        const aiTextTarget = aiMessageWrapper.querySelector('.ai-text-target');
                        
                        // Fill instantly and remove animation class for instant rendering
                        if(aiTextTarget) aiTextTarget.innerText = aiResponse.text;
                        aiMessageWrapper.classList.remove('fade-in-up'); 
                    });
                });
            });

            // 4. Finalize state
            currentTopicIndex = conversationTopics.length;
            currentStepIndex = 0;
            totalHumanMessages = totalHumanMessagesRendered; // Sync global state
            isAiThinking = false;
            
            // 5. Lock the input area
            els.userInput.value = ""; // Ensure value is cleared
            els.userInput.readOnly = true;
            els.userInput.placeholder = "Analysis Complete. Thank you.";
            els.sendBtn.disabled = true;
            els.sendBtn.classList.remove('bg-blue-600', 'text-white', 'hover:bg-blue-700', 'scale-100', 'shadow-md');
            els.sendBtn.classList.add('bg-slate-200', 'text-slate-400', 'scale-90');
            if (els.tooltip) els.tooltip.classList.add('hidden');
            
            // Disable the "Show All" button after use
            document.getElementById('show-all-btn').disabled = true;

            // SCROLL REMOVED: Do not scroll to the very end
        };


        // --- ANIMATION SEQ ---
        function animateTitleOnLoad() {
            els.sidebar.classList.add('is-active');

            typeText(els.titleTarget, NEW_TITLE, 40).then(() => {
                els.subHeader.innerHTML = SUBTITLE_HEADER.replace(/\n/g, '<br>');
                els.subNames.innerHTML = SUBTITLE_NAMES.replace(/\n/g, '<br>'); 
                els.subNames.classList.remove('opacity-0', 'translate-y-4');
                return new Promise(r => setTimeout(r, 500));
            }).then(() => {
                els.sidebarNav.classList.remove('opacity-0');
                return new Promise(r => setTimeout(r, 1000));
            }).then(() => {
                els.initialAiMessage.classList.remove('hidden');
                prepareNextStep();
            });
        }

        // --- EVENT ---
        els.sendBtn.addEventListener('click', async () => {
            // Guard clause: Human can only send if not typing (in animation) and not in the middle of an AI sequence.
            if (isTyping || isAiThinking || currentTopicIndex >= conversationTopics.length) return;

            // --- HUMAN TURN LOGIC ---
            const currentTopic = conversationTopics[currentTopicIndex];
            const stepData = currentTopic.steps[currentStepIndex];
            
            totalHumanMessages++;
            const humanMsgId = totalHumanMessages; 
            const isNewTopicStart = currentStepIndex === 0;

            // 1. Add Human Message
            addMessage('human', stepData.human, 'none', humanMsgId); 

            // Scroll to show the human message right after it's added (since addMessage no longer scrolls)
            scrollToBottom(); 
            
            if (isNewTopicStart) addToSidebar(currentTopicIndex, currentTopic.title, humanMsgId);

            // Lock Input/Button for AI turn
            els.userInput.value = "";
            els.userInput.style.height = 'auto'; // Reset height
            els.userInput.placeholder = "AI is thinking...";
            els.sendBtn.disabled = true;
            els.sendBtn.classList.add('bg-slate-200', 'text-slate-400', 'scale-90');
            if (els.tooltip) els.tooltip.classList.add('hidden');
            
            isAiThinking = true; // Block the send button until the full AI sequence finishes
            toggleTyping(true);

            // 2. Start the AI response sequence (Initial delay before first bubble)
            await new Promise(r => setTimeout(r, 1000)); 
            
            currentAiMessageIndex = 0;
            // This function generates the first bubble and then pauses (via inline button) or completes the sequence.
            await generateNextAiBubble(); 
        });

        window.onload = () => setTimeout(animateTitleOnLoad, 500);

    </script>
</body>
</html>
